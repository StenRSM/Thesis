{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Answering research question 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_pan15_train = pd.read_csv(r'C:\\Users\\Sten\\Documents\\EUR BIM\\thesis\\data\\data\\raw_data\\PAN_15_training.csv')\n",
    "data_pan15_test = pd.read_csv(r'C:\\Users\\Sten\\Documents\\EUR BIM\\thesis\\data\\data\\raw_data\\PAN_15_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pan15_train = data_pan15_train.groupby('author').agg({\n",
    "    'text': ' '.join,\n",
    "    'gender': 'first',\n",
    "    'age': 'first'\n",
    "}).reset_index()\n",
    "combined_pan15_test = data_pan15_test.groupby('author').agg({\n",
    "    'text': ' '.join,\n",
    "    'gender': 'first',\n",
    "    'age': 'first'\n",
    "}).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt = pd.read_csv(r'C:\\Users\\Sten\\Documents\\EUR BIM\\thesis\\data\\data\\gpt1.csv')\n",
    "df_gemini = pd.read_csv(r'C:\\Users\\Sten\\Documents\\EUR BIM\\thesis\\data\\data\\gemini1.csv')\n",
    "df_llama = pd.read_csv(r'C:\\Users\\Sten\\Documents\\EUR BIM\\thesis\\data\\data\\llama1.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variable creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from nltk.probability import FreqDist\n",
    "import numpy as np\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import statistics\n",
    "\n",
    "\n",
    "# Character-based features\n",
    "def character_count(text):\n",
    "    return len(text)\n",
    "\n",
    "def alphabetic_ratio(text):\n",
    "    alphabetic = sum(c.isalpha() for c in text)\n",
    "    return alphabetic/len(text)\n",
    "\n",
    "def uppercase_ratio(text):\n",
    "    upper = sum(c.isupper() for c in text)\n",
    "    return upper/len(text)\n",
    "\n",
    "def digit_ratio(text):\n",
    "    digit = sum(c.isdigit() for c in text)\n",
    "    return digit/len(text)\n",
    "\n",
    "def whitespace_ratio(text):\n",
    "    whitespace = sum(c.isspace() for c in text)\n",
    "    return whitespace/len(text)\n",
    "\n",
    "def tab_ratio(text):\n",
    "    tabs = text.count('\\t')\n",
    "    return tabs/len(text)\n",
    "\n",
    "def letter_ratio(text, letter):\n",
    "    text = text.lower()\n",
    "    letter_count = text.count(letter)\n",
    "    return letter_count/len(text)\n",
    "\n",
    "def specialcharacter_ratio(text, character):\n",
    "    spec_count = text.count(character)\n",
    "    return spec_count/len(text)\n",
    "\n",
    "# Word-based features\n",
    "def number_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    return len(words)\n",
    "\n",
    "def word_length(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return 0\n",
    "\n",
    "    total_length = sum(len(word) for word in words)\n",
    "    return total_length/num_words\n",
    "\n",
    "def vocabulary_richness(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    num_words = len(words)\n",
    "    if num_words == 0:\n",
    "        return 0\n",
    "    \n",
    "    num_uniq_words = len(set(words))\n",
    "    return num_uniq_words/num_words\n",
    "\n",
    "def long_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    long_words_list = [word for word in words if len(word) > 6]\n",
    "    return len(long_words_list)/len(words)\n",
    "\n",
    "def short_words(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text)\n",
    "    short_words_list = [word for word in words if 1 <= len(word) <= 3]\n",
    "    return len(short_words_list)/len(words)\n",
    "\n",
    "def legomena(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    freq = FreqDist(words)\n",
    "    legomena = [word for word in freq if freq[word] == 1]\n",
    "    return len(legomena)/len(words)\n",
    "\n",
    "def dislegomena(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    freq = FreqDist(words)\n",
    "    dislegomena = [word for word in freq if freq[word] == 2]\n",
    "    return len(dislegomena)/len(words)\n",
    "\n",
    "def yules_k(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    freq = FreqDist(words)\n",
    "    N = len(words)\n",
    "    Vi = FreqDist(freq.values())\n",
    "    K = 10**4 * ((-N + sum(i**2 * Vi[i] for i in Vi))/N**2)\n",
    "    return K\n",
    "\n",
    "def simpson_d(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    freq = FreqDist(words)\n",
    "    N = len(words)\n",
    "    if N < 2:\n",
    "        return 0\n",
    "    D = sum(fr * (fr - 1) / (N * (N - 1)) for fr in freq.values())\n",
    "    return D\n",
    "\n",
    "def sichel_s(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    freq = FreqDist(words)\n",
    "    dislegomena = [word for word in freq if freq[word] == 2]\n",
    "    S = len(dislegomena)/len(freq.values())\n",
    "    return S\n",
    "\n",
    "def honores_r(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    if not words:\n",
    "        return 0\n",
    "    freq = FreqDist(words)\n",
    "    N = len(words)\n",
    "    V = len(freq.values())\n",
    "    legomena = [word for word in freq if freq[word] == 1]\n",
    "    unique_count_ratio = len(legomena) / V if V > 0 else 0\n",
    "    if unique_count_ratio == 1 or N == 0:\n",
    "        return 0\n",
    "    R = (100*np.log(N)/(1-(len(legomena)/V)))\n",
    "    return R\n",
    "\n",
    "def entropy(text):\n",
    "    words = re.findall(r'\\b\\w+\\b', text.lower())\n",
    "    freq = FreqDist(words)\n",
    "    N = len(words)\n",
    "    E = -sum((fr / N) * np.log(fr/N) for fr in freq.values())\n",
    "    return E\n",
    "\n",
    "# Syntatic features\n",
    "def punctuations_ratio(text, punctuation):\n",
    "    punctuation_list = re.findall(punctuation, text)\n",
    "    return len(punctuation_list)/len(text)\n",
    "\n",
    "# Structural features\n",
    "def lines(text):\n",
    "    return len(text.split('\\n'))\n",
    "\n",
    "def sentences(text):\n",
    "    return len(sent_tokenize(text))\n",
    "\n",
    "def paragraphs(text):\n",
    "    return len([par for par in text.split('\\n\\n') if par.strip()])\n",
    "\n",
    "def sentence_paragraph(text):\n",
    "    pars = [par for par in text.split('\\n\\n') if par.strip()]\n",
    "    return statistics.mean([len(sent_tokenize(par)) for par in pars])\n",
    "\n",
    "def words_paragraph(text):\n",
    "    pars = [par for par in text.split('\\n\\n') if par.strip()]\n",
    "    return statistics.mean([len(re.findall(r'\\b\\w+\\b', par)) for par in pars])\n",
    "\n",
    "def chars_paragraph(text):\n",
    "    pars = [par for par in text.split('\\n\\n') if par.strip()]\n",
    "    return statistics.mean([len(par) for par in pars])\n",
    "\n",
    "def words_sentences(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    return statistics.mean([len(word_tokenize(sentence)) for sentence in sents])\n",
    "\n",
    "def uppercase_start(text):\n",
    "    sents = sent_tokenize(text)\n",
    "    return (sum(1 for sentence in sents if sentence[0].isupper()) / len(sents))\n",
    "\n",
    "def extract_features(dataframe, text_column):\n",
    "    features = pd.DataFrame()\n",
    "\n",
    "    # Character-based features\n",
    "    features['total_characters'] = dataframe[text_column].apply(character_count)\n",
    "    features['ratio_alphabetic'] = dataframe[text_column].apply(alphabetic_ratio)\n",
    "    features['ratio_uppercase'] = dataframe[text_column].apply(uppercase_ratio)\n",
    "    features['ratio_digit'] = dataframe[text_column].apply(digit_ratio)\n",
    "    features['ratio_whitespace'] = dataframe[text_column].apply(whitespace_ratio)\n",
    "    features['ratio_tabspace'] = dataframe[text_column].apply(tab_ratio)\n",
    "    letters = ['a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
    "    for letter in letters:\n",
    "        features[letter+'_frequency'] = dataframe[text_column].apply(letter_ratio, args=(letter,))\n",
    "    special_characters = ['~', '@', '#', '$', '%', '^', '&', '*', '-', '_', '=', '+', '>', '<', '[', ']', '{', '}', '/', '\\\\', '|']\n",
    "    for character in special_characters:\n",
    "        features[character+'_frequency'] = dataframe[text_column].apply(specialcharacter_ratio, args=(character,))\n",
    "    \n",
    "    # Word-based features\n",
    "    features['total_words'] = dataframe[text_column].apply(number_words)\n",
    "    features['word_length'] = dataframe[text_column].apply(word_length)\n",
    "    features['vocabulary_richness'] = dataframe[text_column].apply(vocabulary_richness)\n",
    "    features['long_words'] = dataframe[text_column].apply(long_words)\n",
    "    features['short_words'] = dataframe[text_column].apply(short_words)\n",
    "    features['hapax_legomena'] = dataframe[text_column].apply(legomena)\n",
    "    features['hapax_dislegomena'] = dataframe[text_column].apply(dislegomena)\n",
    "    features['yules_k'] = dataframe[text_column].apply(yules_k)\n",
    "    features['simpson_d'] = dataframe[text_column].apply(simpson_d)\n",
    "    features['sichel_s'] = dataframe[text_column].apply(sichel_s)\n",
    "    #features['honore_r'] = dataframe[text_column].apply(honores_r)\n",
    "    features['entropy'] =  dataframe[text_column].apply(entropy)\n",
    "    # Brunet W?\n",
    "    # word length frequency distribution\n",
    "\n",
    "    \n",
    "    # Syntactic features\n",
    "    punctuations = [r\"’\", r\",\", r\"\\.\", r\":\", r\";\", r\"\\?\", r\"\\?{2,}\", r\"!\", r\"!{2,}\", r\"\\.{3}\"]\n",
    "    for punctuation in punctuations:\n",
    "        features[punctuation+\"_frequency\"] = dataframe[text_column].apply(punctuations_ratio, args=(punctuation,))\n",
    "\n",
    "    # Structural features\n",
    "    features['number_lines'] = dataframe[text_column].apply(lines)\n",
    "    features['number_sentences'] = dataframe[text_column].apply(sentences)\n",
    "    features['number_paragraphs'] = dataframe[text_column].apply(paragraphs)\n",
    "    features['sentences_per_paragraph'] = dataframe[text_column].apply(sentence_paragraph)\n",
    "    features['word_per_paragraph'] = dataframe[text_column].apply(words_paragraph)\n",
    "    features['character_per_paragraph'] = dataframe[text_column].apply(chars_paragraph)\n",
    "    features['word_per_sentence'] = dataframe[text_column].apply(words_sentences)\n",
    "    features['ratio_sentencestart_uppercase'] = dataframe[text_column].apply(uppercase_start)\n",
    "    #features['gender'] = dataframe['gender']\n",
    "        \n",
    "    return features\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "combined_pan15_train_features = extract_features(combined_pan15_train, 'text')\n",
    "combined_pan15_test_features = extract_features(combined_pan15_test, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "combined_pan15_train_features_scaled = pd.DataFrame(scaler.fit_transform(combined_pan15_train_features), columns=combined_pan15_train_features.columns)\n",
    "combined_pan15_test_features_scaled = pd.DataFrame(scaler.transform(combined_pan15_test_features), columns=combined_pan15_test_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = combined_pan15_train_features_scaled\n",
    "X_test = combined_pan15_test_features_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = combined_pan15_train['gender']\n",
    "y_test = combined_pan15_test['gender']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LinearSVC(C=0.01, max_iter=10000)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LinearSVC</label><div class=\"sk-toggleable__content\"><pre>LinearSVC(C=0.01, max_iter=10000)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LinearSVC(C=0.01, max_iter=10000)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LinearSVC(max_iter=10000, C= 0.01, class_weight= None, loss= 'squared_hinge', tol= 0.0001)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7535211267605634"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "accuracy_score(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training improved model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "from scipy.sparse import csr_matrix, hstack"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "char_vectorizer = TfidfVectorizer(analyzer='char', ngram_range=(3, 5), sublinear_tf=True, min_df=2)\n",
    "word_vectorizer = TfidfVectorizer(analyzer='word', ngram_range=(1, 2), sublinear_tf=True, min_df=2)\n",
    "\n",
    "combined_features = FeatureUnion([\n",
    "    ('char', char_vectorizer),\n",
    "    ('word', word_vectorizer)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf_features_train = combined_features.fit_transform(combined_pan15_train['text'])\n",
    "tfidf_features_test =  combined_features.transform(combined_pan15_test['text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = hstack([tfidf_features_train, csr_matrix(combined_pan15_train_features_scaled)])\n",
    "X_test = hstack([tfidf_features_test, csr_matrix(combined_pan15_test_features_scaled)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.754\n",
      "Precision: 0.743\n",
      "Recall: 0.775\n"
     ]
    }
   ],
   "source": [
    "model = LinearSVC(max_iter=10000, C= 0.01, class_weight= None, loss= 'squared_hinge', tol= 0.0001)\n",
    "model.fit(X_train, y_train)\n",
    "y_pred = model.predict(X_test)\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred), 3))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred, pos_label='F'), 3))\n",
    "print(\"Recall:\", round(recall_score(y_test, y_pred, pos_label='F'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logistic Regression:\n",
      "Accuracy: 0.761\n",
      "Precision: 0.728\n",
      "Recall: 0.831\n"
     ]
    }
   ],
   "source": [
    "model_lr = LogisticRegression()\n",
    "model_lr.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_lr = model_lr.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Logistic Regression:\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_lr), 3))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred_lr, pos_label='F'), 3))\n",
    "print(\"Recall:\", round(recall_score(y_test, y_pred_lr, pos_label='F'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random Forest:\n",
      "Accuracy: 0.789\n",
      "Precision: 0.815\n",
      "Recall: 0.746\n"
     ]
    }
   ],
   "source": [
    "model_rf = RandomForestClassifier()\n",
    "model_rf.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions\n",
    "y_pred_rf = model_rf.predict(X_test)\n",
    "\n",
    "# Print evaluation metrics\n",
    "print(\"Random Forest:\")\n",
    "print(\"Accuracy:\", round(accuracy_score(y_test, y_pred_rf), 3))\n",
    "print(\"Precision:\", round(precision_score(y_test, y_pred_rf, pos_label='F'), 3))\n",
    "print(\"Recall:\", round(recall_score(y_test, y_pred_rf, pos_label='F'), 3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "142"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<152x164169 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1463113 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting LLM data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_features = extract_features(df_gpt, 'text')\n",
    "gemini_features = extract_features(df_gemini, 'text')\n",
    "llama_features = extract_features(df_llama, 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_features_scaled = pd.DataFrame(scaler.transform(gpt_features), columns=gpt_features.columns)\n",
    "gemini_features_scaled = pd.DataFrame(scaler.fit_transform(gemini_features), columns=gemini_features.columns)\n",
    "llama_features_scaled = pd.DataFrame(scaler.transform(llama_features), columns=llama_features.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_features_tfidf = combined_features.transform(df_gpt['text'])\n",
    "gpt_features_combined = hstack([gpt_features_tfidf, csr_matrix(gpt_features_scaled)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'M': 109, 'F': 141})"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gpt_pred = model.predict(gpt_features_combined)\n",
    "Counter(gpt_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "gemini_features_tfidf = combined_features.transform(df_gemini['text'])\n",
    "gemini_features_combined = hstack([gemini_features_tfidf, csr_matrix(gemini_features_scaled)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'F': 67, 'M': 70})"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gemini_pred = model.predict(gemini_features_combined)\n",
    "Counter(gemini_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_features_tfidf = combined_features.transform(df_llama['text'])\n",
    "llama_features_combined = hstack([llama_features_tfidf, csr_matrix(llama_features_scaled)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'F': 120, 'M': 69})"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "llama_pred = model.predict(llama_features_combined)\n",
    "Counter(llama_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "64"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "64"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
